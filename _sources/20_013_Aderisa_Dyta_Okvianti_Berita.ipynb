{"cells":[{"cell_type":"markdown","source":["### Crawling 1 Berita"],"metadata":{"id":"lrOE_oU7EfZ7"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s2tj7aHHL-dn","outputId":"1fc44b91-31af-4e72-d938-3937ebd205eb","executionInfo":{"status":"ok","timestamp":1701092785091,"user_tz":-420,"elapsed":18,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\t\t\t\tJakarta (ANTARA) - Jaksa penuntut umum (JPU) pada Kejaksaan Agung (Kejagung) RI menuntut Account Director of Integrated Account Departement PT Huawei Tech Investment Mukti Ali selama 6 tahun dan membayar denda Rp500 juta subsider 6 bulan pidana kurungan dalam perkara korupsi BTS 4G Kementerian Komunikasi dan Informatika (Kemenkominfo).\n","\"Menjatuhkan pidana terhadap terdakwa Mukti Ali selama 6 tahun dikurangi sepenuhnya dengan lamanya terdakwa ditahan dengan perintah agar terdakwa tetap dilakukan penahanan di rutan,\" kata jaksa dalam persidangan di Pengadilan Tindak Pidana Korupsi (Tipikor) pada Pengadilan Negeri Jakarta Pusat, Senin.\n","Jaksa menyatakan bahwa terdakwa Mukti Ali terbukti secara sah dan meyakinkan bersalah menurut hukum turut serta dalam melakukan tindak pidana korupsi BTS 4G Kementerian Komunikasi dan Informatika (Kemenkominfo).\n","Mukti didakwa telah melanggar Pasal 2 ayat (1) juncto Pasal 18 Undang-Undang Nomor 31 Tahun 1999 tentang Pemberantasan Tindak Pidana Korupsi sebagaimana diubah dengan UU No.\n","20 Tahun 2001 jo.\n","Pasal 55 ayat (1) ke-1 KUHP.\n","Sementara itu, hal-hal yang memberatkan terdakwa adalah perbuatannya tidak mendukung program pemerintah dalam rangka penyelenggaraan negara yang bersih dari korupsi, kolusi, dan nepotisme.\n","\"Perbuatan terdakwa bersama-sama dengan terdakwa lain telah mengakibatkan kerugian keuangan negara sebesar Rp8.032.084.133.795,51,\" ungkap jaksa.\n","Adapun hal-hal yang meringankan Mukti Ali adalah terdakwa belum pernah dihukum, bersikap sopan selama persidangan, dan tidak menikmati hasil dari tindak pidana korupsi.\n","Baca juga: Irwan Hermawan dituntut 6 tahun penjara terkait korupsi BTS 4G\r\n","Baca juga: Galumbang Menak dituntut 15 tahun penjara dalam kasus BTS 4G\r\n"," \n","Sidang tuntutan ini digelar bersamaan dengan tuntutan dua terdakwa lainnya, yakni Komisaris PT Solitech Media Sinergy Irwan Hermawan dan eks Direktur Utama PT Mora Telematika Indonesia Galumbang Menak Simanjuntak.\n","Irwan Hermawan dituntut pidana penjara selama 6 tahun, sementara Galumbang Menak dituntut 15 tahun.\n","Para terdakwa diduga melakukan tindak pidana korupsi penyediaan infrastruktur base transceiver station (BTS) 4G dan infrastruktur pendukung paket 1, 2, 3, 4, dan 5 BAKTI Kemenkominfo pada tahun 2020–2022.\n","Pada surat dakwaan disebutkan bahwa sejumlah pihak mendapat keuntungan dari proyek pembangunan tersebut, yaitu mantan Menteri Kominfo Johnny G. Plate menerima uang sebesar Rp17.848.308.000,00; mantan Direktur Utama Bakti Kominfo Anang Achmad Latif menerima uang Rp5 miliar; dan tenaga ahli Human Development Universitas Indonesia Yohan Suryanto menerima Rp453.608.400,00.\n","Selanjutnya, Irwan Hermawan selaku Komisaris PT Solitechmedia Sinergy menerima Rp119 miliar; Windi Purnama selaku Direktur PT Multimedia Berdikari Sejahtera menerima Rp500 juta; Muhammad Yusrizki selaku Direktur PT Basis Utama Prima menerima Rp50 miliar dan 2,5 juta dolar AS; Konsorsium FiberHome PT Telkominfra PT Multi Trans Data (PT MTD) untuk Paket 1 dan 2 menerima Rp2.940.870.824.490,00; Konsorsium Lintasarta Huawei SEI untuk paket 3 menerima Rp1.584.914.620.955,00; dan Konsorsium IBS dan ZTE Paket 4 dan 5 mendapat Rp3.504.518.715.600,00.Pewarta: Rivan Awal LinggaEditor: D.Dj.\n","Kliwantoro\t\t\t\tCOPYRIGHT © ANTARA 2023\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["#melakukan web scraping pada halaman berita yang berasal dari URL\n","import requests\n","from bs4 import BeautifulSoup\n","import nltk\n","\n","# Unduh konten halaman web berita\n","url = \"https://www.antaranews.com/berita/3799398/mukti-ali-dituntut-6-tahun-penjara\"\n","response = requests.get(url)\n","html = response.text\n","\n","# Parsing halaman web menggunakan BeautifulSoup\n","soup = BeautifulSoup(html, 'html.parser')\n","\n","# Ekstraksi teks dari elemen-elemen yang berisi berita\n","article = soup.find('div', class_=\"post-content clearfix\")  # Sesuaikan dengan struktur HTML halaman web berita\n","\n","# Periksa apakah elemen article ada sebelum mencoba mengambil teksnya\n","if article is not None:\n","    article_text = article.get_text()\n","\n","    # Tokenisasi teks menjadi kalimat menggunakan nltk\n","    nltk.download('punkt')  # Pastikan Anda sudah mengunduh tokenisasi kalimat nltk\n","    sentences = nltk.sent_tokenize(article_text)\n","\n","    # Cetak kalimat-kalimat\n","    for sentence in sentences:\n","        print(sentence)\n","else:\n","    print(\"Elemen berita tidak ditemukan\")\n"]},{"cell_type":"code","source":["!pip install Sastrawi\n","import nltk\n","import warnings\n","import pandas as pd\n","import numpy as np\n","import re\n","import csv\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJ8IvN2XBuz_","executionInfo":{"status":"ok","timestamp":1701093132521,"user_tz":-420,"elapsed":3992,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}},"outputId":"d1b40087-a765-4845-bd6d-397581729f2d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"]}]},{"cell_type":"code","source":["# Preprocessing\n","# Lowercasing\n","article_text = article_text.lower()\n","\n","# Cleaning\n","article_text = ''.join(e for e in article_text if (e.isalnum() or e.isspace() or e == '.'))\n","\n","# Hapus Angka\n","article_text = ''.join([char for char in article_text if not char.isdigit()])\n","\n","# Tokenisasi teks menjadi kalimat menggunakan nltk\n","nltk.download('punkt')\n","sentences = nltk.sent_tokenize(article_text)\n","\n","# Tokenisasi setiap kalimat menjadi kata-kata\n","words = [nltk.word_tokenize(sentence) for sentence in sentences]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ReJashnA7vK","executionInfo":{"status":"ok","timestamp":1701092789341,"user_tz":-420,"elapsed":24,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}},"outputId":"9916d20d-4007-4934-dfde-dba1584f3ce9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["# Stopword Removal\n","\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('indonesian'))\n","filtered_sentences = []\n","\n","\n","for sentence in words:\n","    filtered_sentence = [word for word in sentence if word.lower() not in stop_words]\n","    filtered_sentences.append(filtered_sentence)\n","\n","\n","# Cetak kalimat-kalimat yang telah diproses\n","for filtered_sentence in filtered_sentences:\n","    print(filtered_sentence)\n","\n","# Tutup respons setelah digunakan\n","response.close()"],"metadata":{"id":"jIiOfTLkDtKZ","executionInfo":{"status":"error","timestamp":1701093250770,"user_tz":-420,"elapsed":9,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}},"outputId":"6b652d8c-d309-4caa-8ea0-e72c7161da5b","colab":{"base_uri":"https://localhost:8080/","height":1000}},"execution_count":14,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-afcf16e0558c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'indonesian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfiltered_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eCpKKJ3NIv3","executionInfo":{"status":"aborted","timestamp":1701092789342,"user_tz":-420,"elapsed":17,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[],"source":["#menghitung dan mencetak nilai TF-IDF (Term Frequency-Inverse Document Frequency)\n","#untuk setiap kata dalam setiap kalimat dari teks yang telah di-tokenisasi sebelumnya.\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Inisialisasi penghitung TF-IDF\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","# Hitung TF-IDF\n","tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n","\n","# Daftar kata kunci\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","# Konversi matriks TF-IDF menjadi bentuk yang lebih mudah dibaca\n","tfidf_values = tfidf_matrix.toarray()\n","\n","# Cetak TF-IDF untuk setiap kata dalam setiap kalimat\n","for i, sentence in enumerate(sentences):\n","    print(f\"Kalimat {i + 1}: {sentence}\")\n","    for j, word in enumerate(feature_names):\n","        tfidf_value = tfidf_values[i][j]\n","        if tfidf_value > 0:\n","            print(f\"{word}: {tfidf_value:.4f}\")\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvbsDYfzNobD","executionInfo":{"status":"aborted","timestamp":1701092789343,"user_tz":-420,"elapsed":18,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[],"source":["#menghitung kemiripan kosinus (cosine similarity) antara dua kalimat berdasarkan vektor representasi TF-IDF dari masing-masing kalimat.\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Indeks kalimat yang akan dibandingkan\n","sentence1_index = 0  # Ganti dengan indeks kalimat pertama yang ingin Anda bandingkan\n","sentence2_index = 1  # Ganti dengan indeks kalimat kedua yang ingin Anda bandingkan\n","\n","# Ambil vektor TF-IDF untuk kedua kalimat\n","tfidf_vector1 = tfidf_matrix[sentence1_index]\n","tfidf_vector2 = tfidf_matrix[sentence2_index]\n","\n","# Hitung cosine similarity antara kedua vektor\n","similarity = cosine_similarity(tfidf_vector1, tfidf_vector2)\n","\n","# Cetak hasil cosine similarity\n","print(f\"Cosine Similarity antara Kalimat {sentence1_index + 1} dan Kalimat {sentence2_index + 1}: {similarity[0][0]:.4f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szQfLaHGOSJC","executionInfo":{"status":"aborted","timestamp":1701092789344,"user_tz":-420,"elapsed":18,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[],"source":["#menghitung dan mencetak kemiripan kosinus (cosine similarity) antara semua pasangan kalimat dalam teks yang telah di-tokenisasi\n","\n","# Matriks TF-IDF telah dihitung sebelumnya (tfidf_matrix)\n","# Hitung cosine similarity antara semua pasangan kalimat\n","similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","# Cetak hasil similarity_matrix\n","num_sentences = len(sentences)  # Jumlah kalimat\n","for i in range(num_sentences):\n","    for j in range(i+1, num_sentences):\n","        similarity = similarity_matrix[i][j]\n","        print(f\"Cosine Similarity antara Kalimat {i + 1} dan Kalimat {j + 1}: {similarity:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlYfbbEuOeKR","executionInfo":{"status":"aborted","timestamp":1701092789344,"user_tz":-420,"elapsed":18,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[],"source":["#membuat dan mencetak DataFrame menggunakan hasil perhitungan cosine similarity antara semua pasangan kalimat dalam teks yang telah di-tokenisasi dan matriks TF-IDF\n","import pandas as pd\n","\n","# Matriks TF-IDF telah dihitung sebelumnya (tfidf_matrix)\n","# Hitung cosine similarity antara semua pasangan kalimat\n","similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Nama kolom dan indeks untuk DataFrame\n","sentence_indices = [f\"Kalimat {i + 1}\" for i in range(len(sentences))]\n","\n","# Buat DataFrame dari hasil cosine similarity\n","df = pd.DataFrame(similarity_matrix, columns=sentence_indices, index=sentence_indices)\n","\n","# Cetak DataFrame\n","df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07ABWhcOOimI","executionInfo":{"status":"aborted","timestamp":1701092789347,"user_tz":-420,"elapsed":21,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[],"source":["#membuat dan menampilkan grafik matriks yang menggambarkan cosine similarity antara semua pasangan kalimat dalam teks yang telah di-tokenisasi\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Matriks TF-IDF telah dihitung sebelumnya (tfidf_matrix)\n","# Hitung cosine similarity antara semua pasangan kalimat\n","similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Nama kolom dan indeks untuk DataFrame\n","sentence_indices = [f\"Kalimat {i + 1}\" for i in range(len(sentences))]\n","\n","# Buat DataFrame dari hasil cosine similarity\n","df = pd.DataFrame(similarity_matrix, columns=sentence_indices, index=sentence_indices)\n","\n","# Membuat grafik matriks\n","fig, ax = plt.subplots()\n","cax = ax.matshow(df, cmap='coolwarm')\n","fig.colorbar(cax)\n","\n","# Memberi label pada sumbu X dan Y\n","ax.set_xticks(np.arange(len(df.columns)))\n","ax.set_yticks(np.arange(len(df.index)))\n","ax.set_xticklabels(df.columns, rotation=90)\n","ax.set_yticklabels(df.index)\n","\n","# Menampilkan nilai similarity pada matriks\n","for i in range(len(df.index)):\n","    for j in range(len(df.columns)):\n","        text = ax.text(j, i, f'{df.iat[i, j]:.2f}', ha='center', va='center', color='w')\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ju2FXuW8Ov6J","executionInfo":{"status":"aborted","timestamp":1701092789348,"user_tz":-420,"elapsed":22,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[],"source":["#membangun sebuah graf yang merepresentasikan hubungan antara kalimat dalam teks berdasarkan kemiripan (similarity) antara kalimat-kalimat tersebut.\n","import networkx as nx\n","\n","# Buat grafik dari matriks similarity\n","G = nx.Graph()\n","\n","# Tambahkan simpul (node) ke grafik yang mewakili setiap kalimat\n","for sentence in sentences:\n","    G.add_node(sentence)\n","\n","# Tambahkan tepi (edge) antara kalimat berdasarkan similarity\n","for i in range(len(sentences)):\n","    for j in range(i + 1, len(sentences)):\n","        similarity = df.iloc[i, j]  # Mengambil similarity dari DataFrame\n","        if similarity > 0:\n","            G.add_edge(sentences[i], sentences[j], weight=similarity)\n","\n","# Hitung closeness centrality untuk setiap simpul\n","closeness_centrality = nx.closeness_centrality(G, distance='weight')\n","\n","# Cetak closeness centrality\n","for sentence, centrality in closeness_centrality.items():\n","    print(f\"Closeness Centrality of {sentence}: {centrality:.4f}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"z1-W8bvFI4gf","executionInfo":{"status":"aborted","timestamp":1701092789350,"user_tz":-420,"elapsed":24,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8ue7THpPdoG","executionInfo":{"status":"aborted","timestamp":1701092789350,"user_tz":-420,"elapsed":23,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"outputs":[],"source":["#membuat grafik berarah (Directed Graph - DiGraph) yang merepresentasikan hubungan antara kalimat-kalimat dalam teks berdasarkan kemiripan (cosine similarity)\n","import networkx as nx\n","import pandas as pd\n","\n","# Matriks TF-IDF telah dihitung sebelumnya (tfidf_matrix)\n","# Hitung cosine similarity antara semua pasangan kalimat\n","similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Buat grafik berarah (DiGraph) berdasarkan similarity_matrix\n","G = nx.DiGraph()\n","for i in range(len(similarity_matrix)):\n","    G.add_node(i)  # Tambahkan node dengan indeks numerik\n","\n","for i in range(len(similarity_matrix)):\n","    for j in range(len(similarity_matrix)):\n","        similarity = similarity_matrix[i][j]\n","        if similarity > 0 and i != j:  # Pastikan node tidak menghubungkan dirinya sendiri\n","            G.add_edge(i, j)\n","\n","# Hitung closeness centrality\n","closeness_centrality = nx.closeness_centrality(G)\n","\n","# Visualisasi closeness centrality\n","pos = nx.spring_layout(G)  # Atur layout grafik\n","node_size = [v * 1000 for v in closeness_centrality.values()]  # Ubah ukuran node berdasarkan closeness centrality, dengan faktor pengurangan ukuran\n","\n","nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='b')\n","nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True)\n","nx.draw_networkx_labels(G, pos)\n","\n","plt.show()\n","\n","# Cetak closeness centrality dari yang tertinggi hingga terendah\n","print(\"Closeness Centrality (Dari Tertinggi ke Terendah):\")\n","sorted_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n","\n","for node, closeness in sorted_closeness:\n","    sentence = sentences[node]  # Akses kalimat yang sesuai dengan node\n","    print(f\"Node {node}: Closeness Centrality {closeness:.4f}\")\n","    print(f\"Kalimat: {sentence}\")\n","    print()\n","\n","\n","\n"]},{"cell_type":"code","source":["import networkx as nx\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Matriks TF-IDF telah dihitung sebelumnya (tfidf_matrix)\n","# Hitung cosine similarity antara semua pasangan kalimat\n","similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","# Buat grafik berarah (DiGraph) berdasarkan similarity_matrix\n","G = nx.DiGraph()\n","for i in range(len(similarity_matrix)):\n","    G.add_node(i)  # Tambahkan node dengan indeks numerik\n","\n","for i in range(len(similarity_matrix)):\n","    for j in range(len(similarity_matrix)):\n","        similarity = similarity_matrix[i][j]\n","        if similarity > 0 and i != j:  # Pastikan node tidak menghubungkan dirinya sendiri\n","            G.add_edge(i, j, weight=similarity)  # Gunakan similarity sebagai weight\n","\n","# Hitung PageRank centrality\n","pagerank_centrality = nx.pagerank(G, weight='weight')\n","\n","# Visualisasi PageRank centrality\n","pos = nx.spring_layout(G)  # Atur layout grafik\n","node_size = [v * 10000 for v in pagerank_centrality.values()]  # Ubah ukuran node berdasarkan PageRank, dengan faktor pengurangan ukuran\n","\n","nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='b')\n","nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True)\n","nx.draw_networkx_labels(G, pos)\n","\n","plt.show()\n","\n","# Cetak PageRank centrality dari yang tertinggi hingga terendah\n","print(\"PageRank Centrality (Dari Tertinggi ke Terendah):\")\n","sorted_pagerank = sorted(pagerank_centrality.items(), key=lambda x: x[1], reverse=True)\n","\n","for node, pagerank in sorted_pagerank:\n","    sentence = sentences[node]  # Akses kalimat yang sesuai dengan node\n","    print(f\"Node {node}: PageRank Centrality {pagerank:.4f}\")\n","    print(f\"Kalimat: {sentence}\")\n","    print()\n"],"metadata":{"id":"u01Zn538S3kB","executionInfo":{"status":"aborted","timestamp":1701092789350,"user_tz":-420,"elapsed":23,"user":{"displayName":"20-013 Aderisa Dyta Okvianti","userId":"05985592178631495856"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}